{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# from https://arxiv.org/pdf/1606.02147"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 16, 512, 512]), 2.166924)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expect_shape_to_match(x:torch.Tensor,shape):\n",
    "    assert x.shape[1:] == shape, f\"Expected : {shape} but found: {tuple(x.shape[1:])}\"\n",
    "class HelperModule(nn.Module):\n",
    "    def __init__(self,expected_shape) -> None:\n",
    "        super().__init__()\n",
    "        self.expected_shape = expected_shape\n",
    "    def custom_forward(self,*args,**kwargs):\n",
    "        res = self.forward(*args,**kwargs)\n",
    "        expect_shape_to_match(res[0] if isinstance(res,tuple) else res, self.expected_shape)\n",
    "        return res \n",
    "class BottleNeck(HelperModule):\n",
    "    def __init__(self,inc,outc,expected_shape,type=\"normal\",dilation=1,p=0.1) -> None:\n",
    "        super().__init__(expected_shape)\n",
    "        if type == \"downsampling\":\n",
    "            self.max_pool = nn.MaxPool2d(2,stride=2,return_indices=True)\n",
    "            proj_layers = [nn.Conv2d(inc, outc, kernel_size=2,stride=2,bias=False)]\n",
    "        elif type == \"asymmetric\": \n",
    "            proj_layers = [\n",
    "                nn.Conv2d(inc, outc, kernel_size=(5,1),padding=1, bias=False),\n",
    "                nn.Conv2d(inc, outc, kernel_size=(1,5),padding=1, bias=False),\n",
    "            ]\n",
    "        elif type in [\"normal\",\"dilated\"]: \n",
    "            proj_layers = [nn.Conv2d(inc, outc, kernel_size=1, bias=False)]\n",
    "        elif type == \"upsampling\": \n",
    "            self.main_conv1 = nn.Sequential(\n",
    "                nn.Conv2d(inc, outc, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(outc),\n",
    "            )\n",
    "            self.max_unpool = nn.MaxUnpool2d(2)\n",
    "            proj_layers = [nn.Conv2d(inc, outc, kernel_size=1, bias=False)]\n",
    "        else:\n",
    "            raise Exception(f\"Invalid type {type}\")\n",
    "        if type == \"upsampling\":\n",
    "            conv= nn.ConvTranspose2d(outc,outc,kernel_size=2,stride=2,bias=False)\n",
    "        else:\n",
    "            conv= nn.Conv2d(outc,outc,kernel_size=3,dilation=dilation,padding=dilation,bias=False)\n",
    "            \n",
    "        self.conv_projection = nn.Sequential(\n",
    "            *proj_layers,\n",
    "            nn.BatchNorm2d(outc),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.conv = nn.Sequential(\n",
    "            conv,\n",
    "            nn.BatchNorm2d(outc),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.conv_expansion = nn.Sequential(\n",
    "            nn.Conv2d(outc,outc,kernel_size=1),\n",
    "        )\n",
    "        self.regularizer = nn.Dropout2d(p=p)\n",
    "        self.type = type\n",
    "    def forward(self,x,indices=None,output_size=None):\n",
    "        # residual\n",
    "        x2 = self.conv_projection(x)\n",
    "        x2 = self.conv(x2)\n",
    "        x2 = self.conv_expansion(x2)\n",
    "        x2 = self.regularizer(x2)\n",
    "        # main\n",
    "        if self.type == \"upsampling\":\n",
    "            assert indices is not None\n",
    "            # assert output_size is not None\n",
    "            x1 = self.main_conv1(x)\n",
    "            x1 = self.max_unpool(x1,indices=indices)\n",
    "        elif self.type == \"downsampling\":\n",
    "            x1,indices = self.max_pool(x)\n",
    "            n = x2.shape[1]-x1.shape[1]\n",
    "            x1 = torch.nn.functional.pad(x1,(0,0,0,0,n//2,n//2))\n",
    "        else:\n",
    "            x1 = x\n",
    "            # print(self.max_pool)\n",
    "            # x1 = self.max_pool(x)\n",
    "            pass\n",
    "        x = x1+x2\n",
    "        return (x, indices) if self.type == \"downsampling\" else x\n",
    "class InitialBlock(HelperModule):\n",
    "    def __init__(self, inc, outc, expected_shape) -> None:\n",
    "        super().__init__(expected_shape)\n",
    "        self.conv = nn.Conv2d(3,outc-inc,kernel_size=3,stride=2,padding=1)\n",
    "        self.max_pool = nn.MaxPool2d(2)\n",
    "    def forward(self,x):\n",
    "        x1 = self.conv(x)\n",
    "        x2 = self.max_pool(x)\n",
    "        x = torch.concat((x1,x2),dim=1)\n",
    "        return x\n",
    "class Upsampling(BottleNeck):\n",
    "    def __init__(self, inc, outc, expected_shape) -> None:\n",
    "        super().__init__(inc, outc, expected_shape, \"upsampling\")\n",
    "class ENet(nn.Module):\n",
    "    def __init__(self,C) -> None:\n",
    "        super().__init__()\n",
    "        self.initial = InitialBlock(3, 16, (16, 256, 256))\n",
    "        \n",
    "        self.bottleneck1_0 = BottleNeck(16, 64, (64,128,128),type=\"downsampling\")\n",
    "        self.bottleneck1_x = BottleNeck(64, 64, (64,128,128))\n",
    "        \n",
    "        self.bottleneck2_0 = BottleNeck(64, 128, (128,64,64),type=\"downsampling\",p=0.01)\n",
    "        self.bottleneck2_1 = BottleNeck(128, 128, (128,64,64))\n",
    "        self.bottleneck2_2 = BottleNeck(128, 128, (128,64,64),type=\"dilated\",dilation=2)\n",
    "        self.bottleneck2_3 = BottleNeck(128, 128, (128,64,64),type=\"asymmetric\")\n",
    "        self.bottleneck2_4 = BottleNeck(128, 128, (128,64,64),type=\"dilated\",dilation=4)\n",
    "        self.bottleneck2_5 = BottleNeck(128, 128, (128,64,64))\n",
    "        self.bottleneck2_6 = BottleNeck(128, 128, (128,64,64),type=\"dilated\",dilation=8)\n",
    "        self.bottleneck2_7 = BottleNeck(128, 128, (128,64,64),type=\"asymmetric\")\n",
    "        self.bottleneck2_8 = BottleNeck(128, 128, (128,64,64),type=\"dilated\",dilation=16)\n",
    "\n",
    "        self.bottleneck4_0 = Upsampling(128, 64, (64,128,128))\n",
    "        self.bottleneck4_1 = BottleNeck(64, 64, (64,128,128))\n",
    "        self.bottleneck4_2 = BottleNeck(64, 64, (64,128,128))\n",
    "\n",
    "        self.bottleneck5_0 = Upsampling(64, 16, (16,256,256))\n",
    "        self.bottleneck5_1 = BottleNeck(16, 16, (16,256,256))\n",
    "        \n",
    "        self.full_conv = nn.ConvTranspose2d(\n",
    "            16,\n",
    "            C,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        self.expected_shape = (C,512,512)\n",
    "    def forward(self,x):\n",
    "        bs = x.size(0)\n",
    "        x = self.initial.custom_forward(x)\n",
    "        x, indices1 = self.bottleneck1_0.custom_forward(x)  # downsampling\n",
    "        x = self.bottleneck1_x.custom_forward(x)\n",
    "        x, indices2 = self.bottleneck2_0.custom_forward(x)  # downsampling\n",
    "        x = self.bottleneck2_1.custom_forward(x)\n",
    "        x = self.bottleneck2_2.custom_forward(x)  # dilated 2\n",
    "        x = self.bottleneck2_3.custom_forward(x)  # asymmetric 5\n",
    "        x = self.bottleneck2_4.custom_forward(x)  # dilated 4\n",
    "        x = self.bottleneck2_5.custom_forward(x)\n",
    "        x = self.bottleneck2_6.custom_forward(x)  # dilated 8\n",
    "        x = self.bottleneck2_7.custom_forward(x)  # asymmetric 5\n",
    "        x = self.bottleneck2_8.custom_forward(x)  # dilated 16\n",
    "\n",
    "        x = self.bottleneck4_0.custom_forward(x, indices2)  # upsampling\n",
    "        x = self.bottleneck4_1.custom_forward(x)\n",
    "        x = self.bottleneck4_2.custom_forward(x)\n",
    "\n",
    "        x = self.bottleneck5_0.custom_forward(x, indices1)  # upsampling\n",
    "        x = self.bottleneck5_1.custom_forward(x)\n",
    "\n",
    "        x = self.full_conv(x, output_size=(bs, *self.expected_shape))\n",
    "        return x\n",
    "    def nparams(self):\n",
    "        return sum([p.numel() for p in self.parameters()])/1e6\n",
    "model = ENet(16)\n",
    "model.forward(torch.empty(2,3,512,512)).shape,model.nparams()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
