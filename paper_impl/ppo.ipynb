{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import itertools\n",
    "import gymnasium as gym\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rollout:\n",
    "    def __init__(self,size) -> None:\n",
    "        self.obss = None\n",
    "        self.size = size\n",
    "    def init(self, obs_size):\n",
    "        size = self.size\n",
    "        self.obss = np.zeros((size, obs_size))\n",
    "        self.rewards = np.zeros(size)\n",
    "        self.actions = np.zeros(size)\n",
    "        self.prev_log_prob = np.zeros(size)\n",
    "        self.advantages = np.zeros(size)\n",
    "        self.returns = np.zeros(size)\n",
    "        self.values = np.zeros(size)\n",
    "        self.episode_starts = np.zeros(size)\n",
    "        self.i = 0\n",
    "\n",
    "    def add(self, obs, reward, done, action,log_prob):\n",
    "        if self.obss is None:\n",
    "            self.init(obs.shape[0])\n",
    "        self.obss[self.i] = (obs)\n",
    "        self.rewards[self.i] = (reward)\n",
    "        self.actions[self.i] = (action)\n",
    "        self.episode_starts[self.i] = (done)\n",
    "        self.prev_log_prob[self.i] = (log_prob)\n",
    "        self.i += 1\n",
    "\n",
    "    def get(self, batch_size):\n",
    "        size = self.i\n",
    "        indices = np.random.permutation(size)\n",
    "        obs, returns= self.obss[indices], self.returns[indices]\n",
    "        actions = self.actions[indices]\n",
    "        prev_log_prob = self.prev_log_prob[indices]\n",
    "        advantages = self.advantages[indices]\n",
    "        rewards = self.rewards[indices]\n",
    "        i = 0\n",
    "        def to_batch(arr, dtype=torch.float32):\n",
    "            return torch.tensor(arr[i:i+batch_size], dtype=dtype,device=device)\n",
    "        while i < size:\n",
    "            yield map(to_batch,(obs,returns,actions,prev_log_prob,advantages,rewards))#to_batch(obs), to_batch(rewards), to_batch(dones), to_batch(actions), to_batch(prev_log_prob)\n",
    "            i += batch_size\n",
    "    # def calculate_advantages(self):\n",
    "    #     for t in range(self.obss.shape[0]):\n",
    "    #         pass\n",
    "    def calc_advantage(self):\n",
    "        # https://arxiv.org/pdf/1506.02438\n",
    "        rewards = self.rewards\n",
    "        values = self.values\n",
    "        gamma = 0.99\n",
    "        decay = .97\n",
    "        next_values = np.concatenate([values[1:], [0]])\n",
    "        deltas = [rew+gamma * next_val - val for rew, val,next_val in zip(rewards, values, next_values)]\n",
    "        adv = [deltas[-1]]\n",
    "        for i in reversed(range(len(deltas)-1)):\n",
    "            adv.append(deltas[i] + decay * gamma * adv[-1])\n",
    "        self.advantages = np.array(adv[::-1])\n",
    "        self.returns = self.advantages + self.values\n",
    "\n",
    "    def calculate_advantages(self, last_values: np.ndarray, dones:np.ndarray):\n",
    "        # https://arxiv.org/pdf/1506.02438\n",
    "        gamma, decay,gae_lambda = 0.99, 0.97,.95\n",
    "        last_gae_lam = 0\n",
    "        buffer_size = self.obss.shape[0]\n",
    "        for step in reversed(range(buffer_size)):\n",
    "            if step == buffer_size - 1:\n",
    "                next_non_terminal = 1.0 - dones.astype(np.float32)\n",
    "                next_values = last_values\n",
    "            else:\n",
    "                next_non_terminal = 1.0 - self.episode_starts[step + 1]\n",
    "                next_values = self.values[step + 1]\n",
    "            delta = self.rewards[step] + gamma * next_values * \\\n",
    "                        next_non_terminal - self.values[step]\n",
    "            last_gae_lam = delta + gamma * gae_lambda * next_non_terminal * last_gae_lam\n",
    "            self.advantages[step] = last_gae_lam\n",
    "        self.returns = self.advantages + self.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 689/10000 [00:02<00:29, 315.93it/s]\n"
     ]
    }
   ],
   "source": [
    "class PPO:\n",
    "    # https://arxiv.org/pdf/1707.06347\n",
    "\n",
    "    def __init__(self, env: gym.Env,outc) -> None:\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(np.prod(env.observation_space.shape), 64),nn.Tanh(),\n",
    "            nn.Linear(64, outc),nn.Tanh()\n",
    "        ).to(device)\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(np.prod(env.observation_space.shape), 64),nn.Tanh(),\n",
    "            nn.Linear(64, outc),nn.Tanh()\n",
    "        ).to(device)\n",
    "        self.value_net = nn.Linear(outc,1).to(device)\n",
    "        self.action_net = nn.Linear(outc,env.action_space.n).to(device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(itertools.chain(\n",
    "            self.policy.parameters(),\n",
    "            self.value.parameters(),\n",
    "            self.value_net.parameters(),\n",
    "            self.action_net.parameters(),\n",
    "        ),lr=4e-3)\n",
    "        self.env = env\n",
    "        # self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        module_gains = {\n",
    "            self.policy: np.sqrt(2),\n",
    "            self.value: np.sqrt(2),\n",
    "            self.action_net: 0.01,\n",
    "            self.value_net: 1,\n",
    "        }\n",
    "        import functools\n",
    "        def init(module, gain):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.orthogonal_(module.weight, gain=gain)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.fill_(0.0)\n",
    "\n",
    "        for module, gain in module_gains.items():\n",
    "            module.apply(functools.partial(init, gain=gain))\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, timesteps, timesteps_per_rollout, epochs=2, batch_size=64):\n",
    "        try:\n",
    "            import tqdm\n",
    "            with tqdm.tqdm(total=timesteps) as pbar:\n",
    "                i = 0\n",
    "                while i < timesteps:\n",
    "                    roullout = self.collect_rollout(timesteps_per_rollout)\n",
    "                    for e in range(epochs):\n",
    "                        stop = False\n",
    "                        for batch in roullout.get(batch_size):\n",
    "                            stop = self.train_once(batch)\n",
    "                            if stop: break\n",
    "                        if stop: break\n",
    "                    size = roullout.i\n",
    "                    i += size\n",
    "                    pbar.update(size)\n",
    "        except KeyboardInterrupt as e:\n",
    "            self.env.close()\n",
    "        except Exception as e:\n",
    "            self.env.close()\n",
    "            raise e\n",
    "    def train_once(self,batch:list[torch.Tensor]):\n",
    "        c1 = .5\n",
    "        e = .2\n",
    "        obs,returns,actions,prev_log_prob,A,rewards = batch\n",
    "        # evaluation the past actions\n",
    "        _,log_prob,values = self.forward(obs,actions=actions)\n",
    "        ratio = torch.exp(log_prob - prev_log_prob)\n",
    "        lvf = torch.nn.functional.mse_loss(returns,values)\n",
    "        policy_loss = -torch.min(\n",
    "            ratio*A,\n",
    "            torch.clamp(ratio,1-e,1+e)*A\n",
    "        ).mean()\n",
    "        loss = policy_loss + c1 * lvf # to minimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # print(policy_loss.item(),lvf.item(),rewards.cpu().numpy().sum())\n",
    "        should_stop = (prev_log_prob - log_prob).mean() >= .02\n",
    "        return should_stop\n",
    "\n",
    "    def collect_rollout(self, size):\n",
    "        env = self.env\n",
    "        obs, _ = env.reset()\n",
    "        rollout = Rollout(size)\n",
    "        for _ in range(size):\n",
    "            action,log_prob,values = self.take_action(obs)\n",
    "            obs, reward, terminated, truncated, _= env.step(action.item())\n",
    "            done = terminated or truncated\n",
    "            if done:\n",
    "                obs, _ = env.reset()\n",
    "            rollout.add(obs, reward, done,action,log_prob)\n",
    "            if done:\n",
    "                break\n",
    "        # rollout.calculate_advantages(values,np.array(done))\n",
    "        rollout.calc_advantage()\n",
    "        return rollout\n",
    "        \n",
    "    def forward(self,obs,actions=None)->tuple[torch.Tensor,torch.Tensor,torch.Tensor]:\n",
    "        if obs.ndim == 1:\n",
    "            obs = obs[None]\n",
    "        latent_act = self.policy.forward(obs)\n",
    "        latent_val = self.value.forward(obs)\n",
    "        action_logits = self.action_net.forward(latent_act)\n",
    "        values = self.value_net.forward(latent_val)\n",
    "        dist = torch.distributions.Categorical(logits=action_logits)\n",
    "        if actions is None:\n",
    "            actions = dist.sample((obs.shape[0],))[0]\n",
    "        return actions,dist.log_prob(actions),values.squeeze()\n",
    "        \n",
    "    def take_action(self, obs):\n",
    "        with torch.no_grad():\n",
    "            obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "            a,l,v = self.forward(obs)\n",
    "        return a,l,v\n",
    "        \n",
    "    def play(self,env:gym.Env):\n",
    "        import time\n",
    "        try:\n",
    "            while True:\n",
    "                obs,_ = env.reset()\n",
    "                done = False\n",
    "                i = 0\n",
    "                while not done:\n",
    "                    i+=1\n",
    "                    action,*_ = self.take_action(obs)\n",
    "                    obs, _, terminated, truncated, _= env.step(action.item())\n",
    "                    done = terminated or truncated\n",
    "                    env.render()\n",
    "                    time.sleep(1/30)\n",
    "                print(f\"Episode length: {i}\")\n",
    "        except KeyboardInterrupt as e:\n",
    "            env.close()\n",
    "            pass\n",
    "        \n",
    "# game_name = \"Acrobot-v1\"\n",
    "game_name = \"CartPole-v1\"\n",
    "# ppo = PPO(gym.make(), 64)\n",
    "ppo = PPO(gym.make(game_name), 64)\n",
    "ppo.train(10_000, 2048, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.play(gym.make(game_name, render_mode=\"human\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
