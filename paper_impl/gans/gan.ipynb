{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "import torch,torchvision\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# import torchvision.transforms.functional as VF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  * 2\n",
    "cifar10 = DataLoader(\n",
    "    torchvision.datasets.CIFAR10(\n",
    "        \"./datasets\",\n",
    "        download=True,\n",
    "        transform=T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Normalize((.5,.5,.5),(.5,.5,.5))\n",
    "        ])\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4,\n",
    "    pin_memory=(device==\"cuda\"),\n",
    "    shuffle=True,\n",
    ")\n",
    "mnist = DataLoader(\n",
    "    torchvision.datasets.MNIST(\n",
    "        \"./datasets\",\n",
    "        download=True,\n",
    "        transform=T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Normalize((.5),(.5))\n",
    "        ])\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4,\n",
    "    pin_memory=(device==\"cuda\"),\n",
    "    shuffle=True,\n",
    ")\n",
    "len(cifar10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Forward+Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_tick(dis,g):\n",
    "    v = dis.forward(g)\n",
    "    l = F.binary_cross_entropy(v, torch.ones(v.shape).to(device))\n",
    "    return l\n",
    "def disc_tick(dis,x, g):\n",
    "    v1 = dis.forward(x)\n",
    "    v2 = dis.forward(g)\n",
    "    l1 = F.binary_cross_entropy(v1, torch.ones(v1.shape).to(device))\n",
    "    l2 = F.binary_cross_entropy(v2, torch.zeros(v2.shape).to(device))\n",
    "    return (l1+l2)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Case:\n",
    "    gen:nn.Module\n",
    "    dis:nn.Module\n",
    "    mean:float\n",
    "    std:float\n",
    "    gen_optim:torch.optim.Optimizer\n",
    "    dis_optim:torch.optim.Optimizer\n",
    "    gen_input_size:tuple\n",
    "    gen_imput_dim:int\n",
    "def Conv(inc,outc,ks,act=nn.ReLU(inplace=True),transposed=False,norm=True,**kwargs):\n",
    "    _Conv = nn.ConvTranspose2d if transposed else nn.Conv2d\n",
    "    layers = [_Conv(inc, outc, ks, bias=False, **kwargs),]\n",
    "    if norm:layers += [nn.BatchNorm2d(outc)]\n",
    "    if act:layers += [act]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def setup(version, input_dim, input_size,*args):\n",
    "    gen, dis, gen_optim,dis_optim, m, std = version(input_dim,*args)\n",
    "    return Case(\n",
    "        gen=gen,\n",
    "        dis=dis,\n",
    "        mean=m,\n",
    "        std=std,\n",
    "        gen_optim=gen_optim,\n",
    "        dis_optim=dis_optim,\n",
    "        gen_imput_dim=input_dim,\n",
    "        gen_input_size=input_size,\n",
    "    )\n",
    "def grid(images,nrows):\n",
    "    return torchvision.utils.make_grid(images.detach().cpu(),nrow=nrows,value_range=[-1,1])\n",
    "def test(gen,noise,res):\n",
    "    gen.eval()\n",
    "    with torch.inference_mode():\n",
    "        images = gen.forward(noise)\n",
    "    images = grid(images, noise.size(0))\n",
    "    res.append(images)\n",
    "def sample(size,generator=None,m=0.0,std=1.0):\n",
    "    return torch.normal(m,std, size=size, device=device,generator=generator)\n",
    "def show(imgs,**kwargs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=1,nrows=len(imgs), squeeze=True,**kwargs)\n",
    "    axs = np.atleast_1d(axs)\n",
    "    for img,ax in zip(imgs,axs):\n",
    "        import torchvision.transforms.functional as F\n",
    "        img = F.to_pil_image(img)\n",
    "        ax.imshow(np.asarray(img))\n",
    "        ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "def train(\n",
    "    case:Case,\n",
    "    dataloader,\n",
    "    num_epochs,\n",
    "    test_noise=None,\n",
    "    test_every_n_epochs=0,\n",
    "    test_res = [],\n",
    "):\n",
    "    case.gen.train()\n",
    "    case.dis.train()\n",
    "    try:\n",
    "        for e in range(0,num_epochs):\n",
    "            for X, _ in (bar := tqdm.tqdm(dataloader)):\n",
    "                X = X.to(device)\n",
    "                z = sample((X.size(0), *case.gen_input_size),\n",
    "                           m=case.mean, std=case.std)\n",
    "\n",
    "\n",
    "                case.dis_optim.zero_grad()\n",
    "                g = case.gen.forward(z)\n",
    "                disc_loss = disc_tick(case.dis, X, g.detach())\n",
    "                disc_loss.backward()\n",
    "                case.dis_optim.step()\n",
    "                \n",
    "                case.gen_optim.zero_grad()\n",
    "                gen_loss = gen_tick(case.dis, g)\n",
    "                gen_loss.backward()\n",
    "                case.gen_optim.step()\n",
    "\n",
    "                bar.set_description(\n",
    "                    f\"Epoch {e}:  --  Disc:{disc_loss.item():.4f}, Gen:{gen_loss.item():.4f}\")\n",
    "            if (test_noise is not None) and (e % test_every_n_epochs == 0):\n",
    "                test(case.gen, test_noise, test_res)\n",
    "                case.gen.train()\n",
    "                case.dis.train()\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    return test_res\n",
    "def save(path, case:Case):\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    state = {\n",
    "        \"gen\": case.gen.state_dict(),\n",
    "        \"dist\": case.dis.state_dict(),\n",
    "        \"gen_optim\": case.gen_optim.state_dict(),\n",
    "        \"gen_optim\": case.dis_optim.state_dict(),\n",
    "    }\n",
    "    torch.save(state,path)\n",
    "def load(path, case:Case):\n",
    "    state = torch.load(path)\n",
    "    case.gen.load_state_dict(state[\"gen\"])\n",
    "    case.dis.load_state_dict(state[\"dis\"])\n",
    "    case.gen_optim.load_state_dict(state[\"gen_optim\"])\n",
    "    case.dis_optim.load_state_dict(state[\"dis_optim\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple(gen_in):\n",
    "    lr = 0.0002\n",
    "    m, std = 0, 1\n",
    "    gen = nn.Sequential(\n",
    "        Conv(gen_in, 64, 4),\n",
    "        Conv(64, 64, 4),\n",
    "        Conv(64, 64, 4, stride=2, padding=1),\n",
    "        Conv(64, 64, 4, stride=2, padding=1),\n",
    "        nn.Conv2d(64, 1, (3, 3), padding=1),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Flatten(1, 2),\n",
    "    ).to(device)\n",
    "    dis = nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(h*w, 100), nn.ReLU(),\n",
    "        nn.Linear(100, 1),\n",
    "    ).to(device)\n",
    "    return gen, dis, lr,m,std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcgan(gen_in, channels=3):\n",
    "    def init(m):\n",
    "        if isinstance(m,(nn.Conv2d,nn.ConvTranspose2d)):\n",
    "            nn.init.normal_(m.weight.data,0,0.02)\n",
    "        elif isinstance(m,nn.BatchNorm2d):\n",
    "            nn.init.normal_(m.weight.data,0,0.02)\n",
    "            nn.init.normal_(m.bias.data, 0, 0.02)\n",
    "    # https://arxiv.org/pdf/1511.06434.pdf\n",
    "    m, std = 0, 1\n",
    "    lr = 0.0002\n",
    "    gen = nn.Sequential(\n",
    "        Conv(gen_in, 1024, 4, transposed=True),\n",
    "        Conv(1024, 512, 4, stride=2, padding=1, transposed=True),\n",
    "        Conv(512, 256, 4, stride=2, padding=1, transposed=True),\n",
    "        Conv(256, channels, 4, stride=2, padding=1,\n",
    "             act=nn.Tanh(), transposed=True, norm=False),\n",
    "    ).apply(init).to(device)\n",
    "    dis = nn.Sequential(\n",
    "        Conv(channels, 128, 4,stride=2,padding=1, norm=False, act=nn.LeakyReLU(.2)),\n",
    "        Conv(128, 64, 4, stride=2, padding=1, act=nn.LeakyReLU(.2)),\n",
    "        Conv(64, 32, 4, stride=2, padding=1, act=nn.LeakyReLU(.2)),\n",
    "        Conv(32, 1, 3, stride=2, padding=0, act=None,norm=False),\n",
    "        # Conv(128//8, 1, 4, stride=2, padding=1, act=nn.LeakyReLU(.2)),\n",
    "        # nn.AdaptiveAvgPool2d(1),\n",
    "        nn.Flatten(),\n",
    "        nn.Sigmoid(),\n",
    "    ).apply(init).to(device)\n",
    "    gen_optim = optim.Adam(gen.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    dis_optim = optim.Adam(dis.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    return gen, dis, gen_optim,dis_optim , m, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device).manual_seed(1337)\n",
    "gen_input_dim = 100\n",
    "input_size = (gen_input_dim, 1, 1)\n",
    "dcgan_case = setup(dcgan, gen_input_dim, input_size)\n",
    "noise = sample((10, *input_size), generator=generator,\n",
    "               m=dcgan_case.mean, std=dcgan_case.std)\n",
    "test_res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = train(\n",
    "    dcgan_case,\n",
    "    cifar10,\n",
    "    num_epochs=20,\n",
    "    test_noise=noise,\n",
    "    test_every_n_epochs=2,\n",
    "    test_res=test_res,\n",
    ")\n",
    "show([(t+1)/2 for t in test_res[-10:]], figsize=(13, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(\"./models/dcgan_tanh_v2.pt\", dcgan_case)\n",
    "# TODO add soft interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cgan(gen_in, ):\n",
    "    def init(m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "            nn.init.normal_(m.weight.data, 0, 0.02)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.normal_(m.weight.data, 0, 0.02)\n",
    "            nn.init.normal_(m.bias.data, 0, 0.02)\n",
    "    # https://arxiv.org/pdf/1511.06434.pdf\n",
    "    m, std = 0, 1\n",
    "    lr = 0.0002\n",
    "    class Gen(nn.Module):\n",
    "        def __init__(self, n_classes) -> None:\n",
    "            super().__init__()\n",
    "            self.aa = nn.Sequential(\n",
    "                Conv(gen_in, 1024, 4, transposed=True),\n",
    "                Conv(1024, 512, 4, stride=2, padding=1, transposed=True),\n",
    "                Conv(512, 256, 4, stride=2, padding=1, transposed=True),\n",
    "                Conv(256, 3, 4, stride=2, padding=1,\n",
    "                    act=nn.Tanh(), transposed=True, norm=False),\n",
    "            ).apply(init).to(device)\n",
    "            self.embedding = nn.Embedding(n_classes,10)\n",
    "        def forward(self,batch):\n",
    "            X,y = batch\n",
    "            emb = self.embedding(y)\n",
    "    class Dis(nn.Module):\n",
    "        pass\n",
    "    dis = nn.Sequential(\n",
    "        Conv(3, 128, 4, stride=2, padding=1, norm=False, act=nn.LeakyReLU(.2)),\n",
    "        Conv(128, 64, 4, stride=2, padding=1, act=nn.LeakyReLU(.2)),\n",
    "        Conv(64, 32, 4, stride=2, padding=1, act=nn.LeakyReLU(.2)),\n",
    "        Conv(32, 1, 3, stride=2, padding=0, act=None, norm=False),\n",
    "        # Conv(128//8, 1, 4, stride=2, padding=1, act=nn.LeakyReLU(.2)),\n",
    "        # nn.AdaptiveAvgPool2d(1),\n",
    "        nn.Flatten(),\n",
    "        nn.Sigmoid(),\n",
    "    ).apply(init).to(device)\n",
    "    gen_optim = optim.Adam(gen.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    dis_optim = optim.Adam(dis.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    return gen, dis, gen_optim, dis_optim , m, std"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudatorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
